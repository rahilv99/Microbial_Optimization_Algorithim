{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm7lTDD/m7e+nJxfOaZXHl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahilv99/Microbial_Optimization_Algorithim/blob/main/You_Lab_Growth_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO LIST\n",
        "- ensure the model is designed to display best input factors\n",
        "  - displaying known input factors right now, should display predicted factors\n",
        "- change optimization model to better model the context of the problem\n",
        "  - loss fcn is MSE right now\n",
        "  - implement multiple models, test and use best one\n",
        "- find what training factors to evaluate on (not MSE)\n"
      ],
      "metadata": {
        "id": "gp4eOXxznmVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "VqzXZIgecz-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j6sdkkEcohi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process actual data from csv"
      ],
      "metadata": {
        "id": "80z4TrU1jxOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df_growth = pd.read_csv('')\n",
        "\n",
        "# print(df_bank.info())\n",
        "print('Shape of dataframe:', df_growth.shape)\n",
        "df_growth.head()"
      ],
      "metadata": {
        "id": "DP1ZtBMqj3bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read growth data distrubution\n",
        "- May need to normalize growth factors to be 0 to 1 from min to max"
      ],
      "metadata": {
        "id": "xBvmKfhukgCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = df_growth['numbers'].describe()\n",
        "\n",
        "print(f\"Range of 'numbers' column: {description['min']} - {description['max']}\")\n",
        "print(f\"Standard deviation: {description['std']}\")\n",
        "print(f\"Mean: {description['mean']}\")"
      ],
      "metadata": {
        "id": "Gu2OcXS9kmUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into factor and target data"
      ],
      "metadata": {
        "id": "28vi148BmjEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = df_growth.drop('growth factor', axis=1)\n",
        "target_data = df_growth['growth factor']"
      ],
      "metadata": {
        "id": "BaFMUkCAmoo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dummy data for demo"
      ],
      "metadata": {
        "id": "_nb6UuX5c6GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "input_data = np.random.rand(num_samples, 5)  # 5 input factors\n",
        "target_data = np.random.rand(num_samples, 1)  # Growth factor (target variable)"
      ],
      "metadata": {
        "id": "bM0NDo49c-MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and test data"
      ],
      "metadata": {
        "id": "5dCz6znrmRF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets using scikit-learn\n",
        "input_train, input_val, target_train, target_val = train_test_split(\n",
        "    input_data, target_data, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize input data using scikit-learn\n",
        "scaler = StandardScaler()\n",
        "input_train_scaled = scaler.fit_transform(input_train)\n",
        "input_val_scaled = scaler.transform(input_val)"
      ],
      "metadata": {
        "id": "2S8WPgTpmUmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Optimization Model\n",
        "- Can change # of nodes (increase for high bias, decrease for high variance)\n",
        "- lambda in L2 (increase for high variance, decrease for high bias)\n",
        "- Can add more layers (increase for high bias, decrease for high variance)\n",
        "- May need to import Adam as tf.keras.optimizers.Adam(learning_rate=1e^-3)"
      ],
      "metadata": {
        "id": "Ij4eR8zMdMTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimization model using TensorFlow\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(5,), name='input_layer'),\n",
        "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='hidden_layer_1'),\n",
        "    tf.keras.layers.Dense(1, activation='linear', kernel_regularizer=tf.keras.regularizers.l2(0.01), name='output_layer')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])"
      ],
      "metadata": {
        "id": "_hC4aTcfdOiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "SEC8Bg7idSBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using scikit-learn compatible fit method\n",
        "history = model.fit(input_train_scaled, target_train, epochs=50, batch_size=32,\n",
        "                    validation_data=(input_val_scaled, target_val))"
      ],
      "metadata": {
        "id": "pCO7UXnPdUuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "1LEqm8lVdYW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using test data\n",
        "evaluation_result = model.evaluate(input_val_scaled, target_val)\n",
        "print(f\"Evaluation Result: {evaluation_result}\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Cross Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XjV609DXdbZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Model\n",
        "- Bad runtime complexity, can edit later"
      ],
      "metadata": {
        "id": "bLCGVJNhd2TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the input factors that result in the highest growth factor\n",
        "best_input_factors = None\n",
        "best_growth_factor = -1\n",
        "\n",
        "for i in range(len(input_val_scaled)):\n",
        "    current_input_factors = input_val_scaled[i:i+1, :]\n",
        "    current_growth_factor = model.predict(current_input_factors)[0][0]\n",
        "\n",
        "    if current_growth_factor > best_growth_factor:\n",
        "        best_growth_factor = current_growth_factor\n",
        "        best_input_factors = current_input_factors\n",
        "\n",
        "# Inverse transform to get the original scale of input factors\n",
        "best_input_factors_original_scale = scaler.inverse_transform(best_input_factors)\n",
        "\n",
        "print(\"Best Input Factors (Original Scale):\", best_input_factors_original_scale.flatten())\n",
        "print(\"Corresponding Growth Factor:\", best_growth_factor)"
      ],
      "metadata": {
        "id": "kEYuc7Edd0_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}